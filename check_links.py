#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Skript pro kontrolu nefunkÄnÃ­ch internÃ­ch odkazÅ¯ na webu.
SpouÅ¡tÃ­ se z pÅ™Ã­kazovÃ©ho Å™Ã¡dku s URL sitemapy jako argumentem.

Pokud nalezne nefunkÄnÃ­ odkazy, vypÃ­Å¡e je, uloÅ¾Ã­ do 'broken_links_report.md'
s kontextem (kde byl odkaz nalezen) a skonÄÃ­ s chybovÃ½m kÃ³dem 1.
"""

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import concurrent.futures
import time
import threading
import sys
import collections

# --- HlavnÃ­ nastavenÃ­ ---
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}
MAX_WORKERS = 10
LINK_TIMEOUT = 7

# --- Cache pro jiÅ¾ zkontrolovanÃ© odkazy ---
link_cache = {}
cache_lock = threading.Lock()

# GlobÃ¡lnÃ­ promÄ›nnÃ¡ pro zÃ¡kladnÃ­ domÃ©nu webu
BASE_DOMAIN = ""

def get_sitemap_urls(sitemap_url):
    """NaÄte URL sitemapy a vrÃ¡tÃ­ seznam URL strÃ¡nek."""
    urls = []
    print(f"â„¹ï¸ NaÄÃ­tÃ¡m sitemapu z: {sitemap_url}")
    try:
        response = requests.get(sitemap_url, headers=HEADERS)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'lxml-xml')
        for loc in soup.find_all('loc'):
            urls.append(loc.text)
        print(f"âœ… Nalezeno {len(urls)} URL v sitemapÄ›.")
        return urls
    except requests.exceptions.RequestException as e:
        print(f"âŒ Chyba pÅ™i naÄÃ­tÃ¡nÃ­ sitemapy: {e}", file=sys.stderr)
        return []

def check_link(url):
    """
    Zkontroluje stav jednoho odkazu pomocÃ­ metody GET (maskovÃ¡nÃ­).
    """
    status_code = 0
    message = "OK"
    
    if url.startswith(('mailto:', 'tel:', 'javascript:')) or url.startswith('#'):
        return (url, 0, "SKIPPED")
        
    try:
        response = requests.get(
            url, 
            headers=HEADERS, 
            timeout=LINK_TIMEOUT, 
            allow_redirects=True,
            stream=True 
        )
        status_code = response.status_code
        if status_code >= 400:
            message = "BROKEN"
    except requests.exceptions.Timeout:
        status_code = -1
        message = "ERROR (Timeout)"
    except requests.exceptions.ConnectionError:
        status_code = -2
        message = "ERROR (Connection)"
    except requests.exceptions.RequestException:
        status_code = -3
        message = "ERROR (JinÃ¡ chyba)"
    
    return (url, status_code, message)

def check_page_links(page_url):
    """
    Najde vÅ¡echny INTERNÃ odkazy na strÃ¡nce a vrÃ¡tÃ­ seznam nefunkÄnÃ­ch.
    """
    broken_links_on_page = []
    
    try:
        response = requests.get(page_url, headers=HEADERS, timeout=10)
        if response.status_code >= 400:
            print(f"  -> ğŸš¨ Chyba: SamotnÃ¡ strÃ¡nka '{page_url}' je nefunkÄnÃ­ (Status: {response.status_code})")
            return [(page_url, response.status_code, "BROKEN (Page from sitemap)")]
            
        soup = BeautifulSoup(response.content, 'html.parser')
        links_on_page = set()
        
        for a_tag in soup.find_all('a', href=True):
            href = a_tag['href']
            absolute_url = urljoin(page_url, href)
            absolute_url = urlparse(absolute_url)._replace(fragment="").geturl()
            links_on_page.add(absolute_url)

        if not links_on_page:
            return []

        links_for_executor = []
        
        with cache_lock:
            for url in links_on_page:
                try:
                    parsed_link = urlparse(url)
                    hostname = parsed_link.hostname or ""
                    
                    if parsed_link.scheme not in ('http', 'https') or hostname != BASE_DOMAIN:
                        continue

                    if url in link_cache:
                        status, message = link_cache[url]
                        if message not in ("OK", "SKIPPED"):
                            broken_links_on_page.append((url, status, message))
                    else:
                        links_for_executor.append(url)
                
                except Exception as e:
                    print(f"  -> ! Chyba pÅ™i parsovÃ¡nÃ­ URL: {url} ({e})")
        
        if links_for_executor:
            with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
                futures = [executor.submit(check_link, url) for url in links_for_executor]
                
                for future in concurrent.futures.as_completed(futures):
                    url, status, message = future.result()
                    
                    with cache_lock:
                        link_cache[url] = (status, message)
                        
                    if message not in ("OK", "SKIPPED"):
                        broken_links_on_page.append((url, status, message))

    except requests.exceptions.RequestException as e:
        print(f"  -> âŒ Chyba pÅ™i naÄÃ­tÃ¡nÃ­ strÃ¡nky {page_url}: {e}", file=sys.stderr)
    
    return broken_links_on_page

def main(sitemap_url):
    """HlavnÃ­ funkce skriptu."""
    global BASE_DOMAIN
    
    try:
        BASE_DOMAIN = urlparse(sitemap_url).hostname
        if not BASE_DOMAIN:
             raise ValueError("Nelze extrahovat domÃ©nu z URL sitemapy.")
        print(f"â„¹ï¸ Kontrola internÃ­ch odkazÅ¯ pro domÃ©nu: {BASE_DOMAIN}")
            
    except ValueError as e:
        print(f"âŒ KritickÃ¡ chyba: {e}. Skript nemÅ¯Å¾e pokraÄovat.", file=sys.stderr)
        sys.exit(1)
        
    start_time = time.time()
    page_urls = get_sitemap_urls(sitemap_url)
    
    # *** ZMÄšNA ZDE ***
    # MÃ­sto sady (set) pouÅ¾ijeme slovnÃ­k (dictionary),
    # kde klÃ­Ä je nefunkÄnÃ­ URL a hodnota je sada (set) strÃ¡nek, kde byl nalezen.
    # PouÅ¾ijeme defaultdict pro snadnÄ›jÅ¡Ã­ pÅ™idÃ¡vÃ¡nÃ­.
    all_broken_links_map = collections.defaultdict(set)
    
    if page_urls:
        for i, page_url in enumerate(page_urls):
            print(f"\nğŸ” Kontroluji strÃ¡nku ({i+1}/{len(page_urls)}): {page_url}")
            
            broken_links = check_page_links(page_url)
            
            if broken_links:
                print(f"  ğŸš¨ Nalezeny nefunkÄnÃ­ odkazy:")
                for url, status, msg in broken_links:
                    print(f"     -> {url} (Status: {status}, DÅ¯vod: {msg})")
                    # *** ZMÄšNA ZDE ***
                    # UloÅ¾Ã­me si, Å¾e 'url' (nefunkÄnÃ­) byla nalezena na 'page_url' (aktuÃ¡lnÃ­ strÃ¡nka)
                    all_broken_links_map[url].add(page_url)
            else:
                print("  âœ… VÅ¡echny internÃ­ odkazy se zdajÃ­ bÃ½t v poÅ™Ã¡dku.")
                
        end_time = time.time()
        print("\n" + "="*40)
        print("--- ğŸ KONTROLA DOKONÄŒENA (SOUHRN) ---")
        print(f"Celkem zkontrolovÃ¡no strÃ¡nek: {len(page_urls)}")
        print(f"Celkem unikÃ¡tnÃ­ch internÃ­ch odkazÅ¯ zkontrolovÃ¡no (v cache): {len(link_cache)}")
        print(f"CelkovÃ½ Äas: {end_time - start_time:.2f} sekund")
        
        print("\n" + "="*40)
        
        # *** ZMÄšNA ZDE ***
        # ZmÄ›nili jsme logiku reportovÃ¡nÃ­, aby pouÅ¾Ã­vala novou mapu
        if all_broken_links_map:
            print(f"ğŸš¨ğŸš¨ğŸš¨ NALEZENY CHYBY ğŸš¨ğŸš¨ğŸš¨")
            print(f"Celkem nalezeno unikÃ¡tnÃ­ch nefunkÄnÃ­ch internÃ­ch odkazÅ¯: {len(all_broken_links_map)}")
            print("--- Seznam vÅ¡ech unikÃ¡tnÃ­ch nefunkÄnÃ­ch odkazÅ¯ a jejich zdrojÅ¯ ---")

            try:
                with open("broken_links_report.md", "w", encoding="utf-8") as f:
                    f.write(f"# ğŸš¨ Nalezeny nefunkÄnÃ­ odkazy ({len(all_broken_links_map)})\n\n")
                    f.write("BÄ›hem automatickÃ© kontroly webu byly nalezeny nÃ¡sledujÃ­cÃ­ nefunkÄnÃ­ internÃ­ odkazy:\n\n")
                    
                    # SeÅ™adÃ­me podle nefunkÄnÃ­ho odkazu
                    for broken_url, pages in sorted(all_broken_links_map.items()):
                        print(f"\n-> NEFUNKÄŒNÃ ODKAZ: {broken_url}")
                        # PouÅ¾ijeme Markdown nadpis pro pÅ™ehlednost v Issue
                        f.write(f"## âŒ `{broken_url}`\n\n") 
                        f.write("**Nalezeno na tÄ›chto strÃ¡nkÃ¡ch:**\n")
                        print("   Nalezeno na:")
                        
                        for page in sorted(list(pages)):
                            print(f"   - {page}")
                            f.write(f"- {page}\n")
                        f.write("\n") # PÅ™idÃ¡ mezeru pÅ™ed dalÅ¡Ã­m odkazem
                            
                print("\nâ„¹ï¸ Report o chybÃ¡ch byl uloÅ¾en do souboru broken_links_report.md")
            except Exception as e:
                print(f"Chyba pÅ™i zÃ¡pisu reportu do souboru: {e}", file=sys.stderr)
            
            print("="*40)
            sys.exit(1) # VracÃ­me chybovÃ½ kÃ³d
        else:
            print("ğŸ‰ğŸ‰ğŸ‰ VÃBORNÄš! ğŸ‰ğŸ‰ğŸ‰")
            print("Å½Ã¡dnÃ© unikÃ¡tnÃ­ nefunkÄnÃ­ internÃ­ odkazy nebyly nalezeny.")
            print("="*40)
            sys.exit(0) # VracÃ­me kÃ³d 0 (ÃºspÄ›ch)
    else:
        print("Nebyla nalezena Å¾Ã¡dnÃ¡ URL v sitemapÄ›. Kontrola konÄÃ­.", file=sys.stderr)
        sys.exit(1)

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Chyba: MusÃ­te zadat URL sitemapy jako argument.", file=sys.stderr)
        print("PÅ™Ã­klad: python check_links.py https://web.cz/sitemap.xml", file=sys.stderr)
        sys.exit(1)
    
    main(sitemap_url=sys.argv[1])
