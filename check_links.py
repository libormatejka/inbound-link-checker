#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Skript pro kontrolu nefunkÄnÃ­ch internÃ­ch odkazÅ¯ na webu.
SpouÅ¡tÃ­ se z pÅ™Ã­kazovÃ©ho Å™Ã¡dku s URL sitemapy jako argumentem.
Pokud nalezne nefunkÄnÃ­ odkazy, vypÃ­Å¡e je a skonÄÃ­ s chybovÃ½m kÃ³dem 1.
"""

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import concurrent.futures
import time
import threading
import sys
import os

# --- HlavnÃ­ nastavenÃ­ ---
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/5.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/5.36'
}
# PoÄet soubÄ›Å¾nÃ½ch vlÃ¡ken
MAX_WORKERS = 10
# Timeout pro jednotlivÃ© dotazy
LINK_TIMEOUT = 7

# --- Cache pro jiÅ¾ zkontrolovanÃ© odkazy ---
# UklÃ¡dÃ¡: { 'url': (status, message) }
link_cache = {}
cache_lock = threading.Lock()

# GlobÃ¡lnÃ­ promÄ›nnÃ¡ pro zÃ¡kladnÃ­ domÃ©nu webu
BASE_DOMAIN = ""

def get_sitemap_urls(sitemap_url):
    """NaÄte URL sitemapy a vrÃ¡tÃ­ seznam URL strÃ¡nek."""
    urls = []
    print(f"â„¹ï¸ NaÄÃ­tÃ¡m sitemapu z: {sitemap_url}")
    try:
        response = requests.get(sitemap_url, headers=HEADERS)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'lxml-xml')
        for loc in soup.find_all('loc'):
            urls.append(loc.text)
        print(f"âœ… Nalezeno {len(urls)} URL v sitemapÄ›.")
        return urls
    except requests.exceptions.RequestException as e:
        print(f"âŒ Chyba pÅ™i naÄÃ­tÃ¡nÃ­ sitemapy: {e}", file=sys.stderr)
        return []

def check_link(url):
    """
    Zkontroluje stav jednoho odkazu pomocÃ­ metody GET (maskovÃ¡nÃ­).
    """
    status_code = 0
    message = "OK"
    
    if url.startswith(('mailto:', 'tel:', 'javascript:')) or url.startswith('#'):
        return (url, 0, "SKIPPED")
        
    try:
        response = requests.get(
            url, 
            headers=HEADERS, 
            timeout=LINK_TIMEOUT, 
            allow_redirects=True,
            stream=True 
        )
        status_code = response.status_code
        if status_code >= 400:
            message = "BROKEN"
    except requests.exceptions.Timeout:
        status_code = -1
        message = "ERROR (Timeout)"
    except requests.exceptions.ConnectionError:
        status_code = -2
        message = "ERROR (Connection)"
    except requests.exceptions.RequestException:
        status_code = -3
        message = "ERROR (JinÃ¡ chyba)"
    
    return (url, status_code, message)

def check_page_links(page_url):
    """
    Najde vÅ¡echny INTERNÃ odkazy na strÃ¡nce a vrÃ¡tÃ­ seznam nefunkÄnÃ­ch.
    """
    broken_links_on_page = []
    
    try:
        response = requests.get(page_url, headers=HEADERS, timeout=10)
        if response.status_code >= 400:
            print(f"  -> âš ï¸ VarovÃ¡nÃ­: SamotnÃ¡ strÃ¡nka '{page_url}' je nefunkÄnÃ­ (Status: {response.status_code}), pÅ™eskoÄeno.")
            return []
            
        soup = BeautifulSoup(response.content, 'html.parser')
        links_on_page = set()
        
        for a_tag in soup.find_all('a', href=True):
            href = a_tag['href']
            absolute_url = urljoin(page_url, href)
            absolute_url = urlparse(absolute_url)._replace(fragment="").geturl()
            links_on_page.add(absolute_url)

        if not links_on_page:
            return []

        links_for_executor = []
        
        with cache_lock:
            for url in links_on_page:
                try:
                    parsed_link = urlparse(url)
                    hostname = parsed_link.hostname or ""
                    
                    # --- FILTROVÃNÃ: POUZE INTERNÃ ---
                    # PÅ™eskoÄÃ­me vÅ¡e, co nenÃ­ http/https nebo nenÃ­ na naÅ¡Ã­ domÃ©nÄ›
                    if parsed_link.scheme not in ('http', 'https') or hostname != BASE_DOMAIN:
                        continue
                    # --- Konec filtru ---

                    if url in link_cache:
                        status, message = link_cache[url]
                        if message not in ("OK", "SKIPPED"):
                            broken_links_on_page.append((url, status, message))
                    else:
                        links_for_executor.append(url)
                
                except Exception as e:
                    print(f"  -> ! Chyba pÅ™i parsovÃ¡nÃ­ URL: {url} ({e})")
        
        if links_for_executor:
            with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
                futures = [executor.submit(check_link, url) for url in links_for_executor]
                
                for future in concurrent.futures.as_completed(futures):
                    url, status, message = future.result()
                    
                    with cache_lock:
                        link_cache[url] = (status, message)
                        
                    if message not in ("OK", "SKIPPED"):
                        broken_links_on_page.append((url, status, message))

    except requests.exceptions.RequestException as e:
        print(f"  -> âŒ Chyba pÅ™i naÄÃ­tÃ¡nÃ­ strÃ¡nky {page_url}: {e}", file=sys.stderr)
    
    return broken_links_on_page

def main(sitemap_url):
    """HlavnÃ­ funkce skriptu."""
    global BASE_DOMAIN
    
    try:
        BASE_DOMAIN = urlparse(sitemap_url).hostname
        if not BASE_DOMAIN:
             raise ValueError("Nelze extrahovat domÃ©nu z URL sitemapy.")
        print(f"â„¹ï¸ Kontrola internÃ­ch odkazÅ¯ pro domÃ©nu: {BASE_DOMAIN}")
            
    except ValueError as e:
        print(f"âŒ KritickÃ¡ chyba: {e}. Skript nemÅ¯Å¾e pokraÄovat.", file=sys.stderr)
        sys.exit(1)
        
    start_time = time.time()
    page_urls = get_sitemap_urls(sitemap_url)
    
    all_broken_links_set = set()
    
    if page_urls:
        for i, page_url in enumerate(page_urls):
            print(f"\nğŸ” Kontroluji strÃ¡nku ({i+1}/{len(page_urls)}): {page_url}")
            
            broken_links = check_page_links(page_url)
            
            if broken_links:
                print(f"  ğŸš¨ Nalezeny nefunkÄnÃ­ odkazy:")
                for url, status, msg in broken_links:
                    print(f"     -> {url} (Status: {status}, DÅ¯vod: {msg})")
                    all_broken_links_set.add(url)
            else:
                print("  âœ… VÅ¡echny internÃ­ odkazy se zdajÃ­ bÃ½t v poÅ™Ã¡dku.")
                
        end_time = time.time()
        print("\n" + "="*40)
        print("--- ğŸ KONTROLA DOKONÄŒENA (SOUHRN) ---")
        print(f"Celkem zkontrolovÃ¡no strÃ¡nek: {len(page_urls)}")
        print(f"Celkem unikÃ¡tnÃ­ch internÃ­ch odkazÅ¯ zkontrolovÃ¡no (v cache): {len(link_cache)}")
        print(f"CelkovÃ½ Äas: {end_time - start_time:.2f} sekund")
        
        print("\n" + "="*40)
        
        if all_broken_links_set:
            print(f"ğŸš¨ğŸš¨ğŸš¨ NALEZENY CHYBY ğŸš¨ğŸš¨ğŸš¨")
            print(f"Celkem nalezeno unikÃ¡tnÃ­ch nefunkÄnÃ­ch internÃ­ch odkazÅ¯: {len(all_broken_links_set)}")
            print("--- Seznam vÅ¡ech unikÃ¡tnÃ­ch nefunkÄnÃ­ch odkazÅ¯ ---")
            for broken_url in sorted(list(all_broken_links_set)):
                print(f"-> {broken_url}")
            print("="*40)
            # VracÃ­me chybovÃ½ kÃ³d, aby GitHub Action selhala
            sys.exit(1)
        else:
            print("ğŸ‰ğŸ‰ğŸ‰ VÃBORNÄš! ğŸ‰ğŸ‰ğŸ‰")
            print("Å½Ã¡dnÃ© unikÃ¡tnÃ­ nefunkÄnÃ­ internÃ­ odkazy nebyly nalezeny.")
            print("="*40)
            # VracÃ­me kÃ³d 0 (ÃºspÄ›ch)
            sys.exit(0)
    else:
        print("Nebyla nalezena Å¾Ã¡dnÃ¡ URL v sitemapÄ›. Kontrola konÄÃ­.", file=sys.stderr)
        sys.exit(1) # SelhÃ¡nÃ­, pokud se nenaÄte sitemapa

if __name__ == "__main__":
    # PÅ™eÄteme URL sitemapy z prvnÃ­ho argumentu
    if len(sys.argv) < 2:
        print("Chyba: MusÃ­te zadat URL sitemapy jako argument.", file=sys.stderr)
        print("PÅ™Ã­klad: python check_links.py https://web.cz/sitemap.xml", file=sys.stderr)
        sys.exit(1)
    
    main(sitemap_url=sys.argv[1])
